\documentclass[a4paper,11pt]{kth-mag}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{modifications}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[backend=bibtex,style=authoryear]{biblatex}
\addbibresource{references.bib}

\title{Anti-analysis techniques to preserve programmer anonymity in binary
executables}

\subtitle{
    DEGREE PROJECT IN COMPUTER SCIENCE, FIRST LEVEL \\
    STOCKHOLM, SWEDEN 2016
}
\author{Johan Wikström \\ Macaully Muir}
\date{April 2016}
\blurb{
    Supervisor: Michael Schliephake \\
    Examiner: Örjan Ekeberg
}
\trita{TRITA TK: xxx yyyy-nn}
\begin{document}
\frontmatter
\pagestyle{empty}
\removepagenumbers
\maketitle
\selectlanguage{english}
\begin{abstract}
Previous research has shown that it is possible to achieve authorship
attribution with high accuracy using binary files. The idea behind authorship
attribution for binaries is that authors’ unique style will survive the
compilation process making it possible to identify the author.    

The aim of this report is to investigate different ways to reduce the accuracy
and see how the processing of binaries affects the author prediction accuracy.

This work will build on the report of Rosenblum et al. and implement the method
they used. The method uses  a machine learning approach to predict the author
of the binary. The data used for prediction consists of features extracted from
the binaries. Similar to \parencite{rosenblum2011wrote} binary files used in this
thesis come from the Google Code Jam programming competition.

Results from this study indicate that optimisation of code reduces the accuracy
achieved; however, is not enough to ensure anonymity. Using static linking
results in a more significant drop in accuracy. The data extracted shows that
optimisation and static linking results in more features that may be the cause
of the reduction in accuracy.
\end{abstract}
\clearpage
\begin{foreignabstract}{swedish}
I tidigare studier har det visat sig möjligt att med hög noggrannhet
identifiera författaren till binära filer. Idéen är att författare till kod
lämnar karaktäristiska drag i koden som går att känna igen även efter
kompilering. 

Syftet med denna rapport är att utforska olika tekniker för att minska
säkerheten då man vill bestämma vem som är författaren till binära filer och
undersöka hur olika bearbetningar av koden påverkar noggrannheten.     

Denna rapport bygger vidare på rapport av \parencite{rosenblum2011wrote}
och dess metod. En maskininlärningsmetod implementerats för
att identifiera vem som är författaren till en binär fil. Attribut extraheras
från binärfilerna som sedan kan används som data för maskininlärning. Likt
Rosenblum et al. används data hämtat från programmeringstävlingen Google Code
Jam. 

Erhållna resultat tyder på att optimering försämrar noggrannheten men är dock
inte tillräckligt för att garantera anonymitet. Användning av statisk länkning
resulterar i en kraftigare sänkning. Man kan se att optimering och statisk
länkning resulterar i fler attribut vilket kan vara orsaken till att det blir
svårare att avgöra vem som är författaren.
\end{foreignabstract}
\clearpage
\tableofcontents*
\mainmatter
\pagestyle{newchap}

\chapter{Introduction}
Is it possible to extract information about programming style from compiled
binary programs? Can such information be used to identify the author of a given
program? In 2011 Rosenblum et al. showed that the answer to both these
questions is “yes” under certain conditions \parencite{rosenblum2011wrote};
from a pool of 191 Google Code Jam (GCJ) contestants with eight or more
submitted solutions, authorship identification from a set of 10 authors could
be performed with 81\% accuracy using a supervised support vector machine
classifier. In 2015 Caliskan-Islam et al. proposed an alternate method that
could achieve 96\% accuracy from a pool of 20 contestants
\parencite{caliskan2015coding} under similar conditions.

However, it is not clear whether the above results could be replicated with
other datasets from meaningful real-world scenarios. Nevertheless, they do
raise questions about the ability of programmers to write and distribute
software anonymously and it is therefore desirable to find methods that prevent
de-anonymisation with this kind of analysis. Using the methods of
\parencite{rosenblum2011wrote}, this report aims to investigate how changes to
the build process can be used to reduce the accuracy of author classification
on the same Google Code Jam dataset.

\section{Problem definition} In short, this thesis aims to:
\begin{itemize}
\item Validate the results of \parencite{rosenblum2011wrote} on the 2010 Google Code Jam dataset
\item Investigate how g++’s optimisation flags (O0-O3) affect the accuracy of the
      author classification algorithm on the 2010 GCJ dataset
\item Investigate how statically linking the standard library affects the accuracy of
      the author classification algorithm on the 2010 GCJ dataset.
\end{itemize}

\section{Motivation}
There are a number of legitimate reasons why the author of a piece of software
would want to remain anonymous. It could be that their association with the
software could put them in danger (for example, a programmer that writes
anti-surveillance software in an oppressive state). It could also be the case
that the programmer simply does not want to be publicly associated with the
software such as in the cases of TrueCrypt, Bitcoin etc. (TK: source). It is
therefore important to investigate the robustness of these author
classification algorithms and develop tools and procedures that ensure can help
ensure the anonymity of the programmer.

\section{Scope}
This report will only implement the method of Rosenblum et al. by using
available source code when possible and by implement remaining part ourselves
trying to be consistent with the original implementation.  Furthermore, we
limit our analysis to a subset of C++ solutions from the 2010 rounds of Google
Code Jam. This allows us to verify the results in the literature
\parencite{rosenblum2011wrote} and provides a meaningful data set to work with.
This method assume that a program has exactly one author. However, this is
often not true since many programs are developed in teams.

\chapter{Background}
This section begins with an overview of the field. We then introduce the
technical concepts required to understand the rest of this thesis, and finally
present the state-of-the-art in the field of authorship attribution. Focus will
be on deriving authorship from binary programs; however, some related works
will be mentioned.

\section{Overview of the field}
Computer forensics is a branch where the goal is to to analyse digital data to
obtain information, often in relation to computer crime
\parencite{reith2002examination}.  One application is author attribution. In
recent years machine learning techniques have been used with great success to
identify counterfeited integrated circuits \parencite{huang2013counterfeit},
distinguishing different kinds of brain tumours
\parencite{zacharaki2009classification} and the automated detection of software
bugs \parencite{aleem2015comparative}, to name a few examples.

The first attempt at using statistical methods to de-anonymise authors of
programs from their coding style was performed by Oman et al.
\parencite{oman1989programming} who classified programmers using a hand-picked,
language-specific feature set. In 2006, Frantzeskou et al. proposed a method
using the analysis of byte level n-grams (continuous sequences of n bytes)
present in the source code which greatly improved upon previous methods, along
with being language-agnostic \parencite{frantzeskou2006source}. In 2015,
Caliskan-Islam et al. presented a method based on features of the abstract
syntax trees (ASTs) of C/C++ programs that significantly improved the
state-of-the-art again \parencite{caliskan2015anonymizing}.
 
The above methods all assume that de-anonymisation is to be performed on the
source code of a program. However, there are many contexts where
de-anonymisation is desirable where access to the source code is either
impractical or entirely impossible, for example during malware analysis. In
2011 Rosenblum et al. showed that this is is feasible in certain conditions
using machine learning techniques on the stylistic features of source code that
survive the compilation process \parencite{rosenblum2011wrote}. Specifically,
their features were derived from the control flow graph (CFG) of the compiled
binary and byte level n-grams. In 2014 Alrabaee et al. improved upon the
accuracy of those techniques and proposed features that were more closely
related to programming style \parencite{alrabaee2014oba2}. In 2015
Caliskan-Islam et al. further improved the state of the art using disassemblers
and decompilers, and applying their research in de-anonymising programmers from
source code to the ASTs of the decompiled binaries
\parencite{caliskan2015coding}.

\section{Technical Background}
This section includes the technical details necessary for understanding the rest of this thesis. We start with a brief overview of the compilation process and the static analysis, techniques used to extract features from binary executables and a short explanation of support vector machines (SVMs) and their uses. 

\subsection{The compilation process}
 Compilation is the process of transforming source code into executable programs. It is beyond the scope of this thesis to give a complete overview of the inner workings of compilers, so we work under the simplification that compilation is a two step process: assembly code generation and machine code generation.

[Figure]

As illustrated in figure 2.TK, stylistic elements present in source code are
not easily discernible from the resulting binary code. This can be formalised
with the observation that compilation is in general non-reversible; there are
an infinite number of source code programs that would generate a given binary
executable. As such the compilation process suffers from information loss,
since information contained within the source code may not be present in the
resulting binary. Figure 2.TK shows two different source code representations
of the same binary program.

[Figure]

However, it is hard to say exactly what information is preserved in the
compilation process. The compiler guarantees that the semantics of the source
code be preserved in the resulting machine code but its algorithms and data
structures may not. Figure 2.TK shows how gcc optimises the calculation of an
arithmetic sum, replacing the loop with the constant-time formula. The results
of \parencite{rosenblum2011wrote}, \parencite{caliskan2015coding}, and
\parencite{alrabaee2014oba2} are novel in this regard as they show that
stylistic features present in source code do in fact consistently survive the
compilation process.

[Figure]
\subsection{Static analysis of compiled binaries}
Static binary analysis is the process of understanding how a binary program
will behave without executing it. This can involve any number of techniques
but is often used in the context of reverse engineering, that is, the goal of
the analysis is to understand not only the behaviour but also the
implementation of the program. It is assumed that the source code of the
program is not accessible. In this section we present two methods of static
analysis, disassembling and control flow analysis, that are used in our
analysis of the Google Code Jam submissions.

Disassembling is the task of transforming binary code into the equivalent
assembly code and is performed by a disassembler. In general this is a
non-trivial task, especially on machine code for complex instruction sets such
as x86 \parencite{wartell2011differentiating}. Moreover, a number of techniques
can be employed to purposely thwart commonly used disassembly algorithms,
making the process much harder \parencite{linn2003obfuscation}. For the purpose
of our analysis however this is not an issue, as we have found that the
compiled Google Code Jam solutions in our data set can in fact be automatically
disassembled. Figure 2.TK shows a simple assembly routine, its equivalent
machine code after compilation and the resulting assembly code after
disassembly.

[figure]

Control flow analysis is performed on assembly code produced by a disassembler.
The goal is to partition the code into so-called “basic blocks”: sequences of
instructions with exactly one entry point and exit point with respect to
program flow \parencite{allen1970control}. This structure is captured in the
control flow graph (CFG), a directed graph with the basic blocks as nodes and
control flow paths as edges. Figure 2.TK illustrates the basic blocks of a
simple assembly code routine.

[figure]

The CFG provides a structured view of the binary at a higher level than
individual instructions. This can be used to identify high-level language
constructs such as loops and functions \parencite{cifuentes1993methodology} and
as such is used by decompilers such as the Hex-Rays decompiler (TK: Hex-Rays,
2016), used by Caliskan-Islam et al. in their paper improving on the work of
Rosenblum et al \parencite{caliskan2015coding}. Rosenblum et al. also use the
CFG for feature extraction, but do not go as far as to decompile the binaries
\parencite{rosenblum2011wrote}.

\subsection{Support vector machines}
Support vector machines (SVMs) belong to a group of supervised learning methods
and are based on statistical learning theory. SVMs have been around since the
1960s and work well on for example text and image recognition. The learning
algorithm is given data and learns from that to identify patterns or classify
new data. There is a balance in how much data the algorithm should have. Using
too much information can lead to “overfitting” and the algorithm will perform
poorer for new data (TK: Weston, n.d.). The goal is to maximize the distance
between data points in the different classes \parencite{awad2004effective}.

\subsection{Cross-validation}
Cross-validation is a common method used when testing predictions. It is a
model used to analyze how well a predictive model will work in practice. When
using 10-fold cross-validation the data is randomly divided into 10 equally
sized sets, 9 of the sets is used for training and the remaining set it used
for the testing. This is repeated for each one of the sets, finally resulting
in an average measure of accuracy in our case \parencite{hsu2003practical}.        

\section{State-of-the-art analysis}
As previously discussed, there are as far as we are aware of exactly three
papers presenting novel techniques to de-anonymise programmers from compiled
binary programs: \parencite{rosenblum2011wrote}, \parencite{alrabaee2014oba2}
and \parencite{caliskan2015coding}. In this section we discuss each approach.
The focus will be on the techniques used by Rosenblum et al.

\subsection{Rosenblum et al.}
\subsubsection{General approach}
Step 1: Binary disassembly
As a first step, the compiled binary is disassembled into human-readable
assembly code. It is implicitly assumed that this can always be performed, but
it is important to note that this is not necessarily the case. Malware, for
example, is often obfuscated in ways that make automated disassembly difficult
to perform \parencite{branco2012scientific}. Rosenblum’s approach will not work
if the binary cannot be disassembled.

Step 2: Idiom generation
The disassembled code is parsed and all sequences of 1-3 instructions are
extracted and counted. Specifically, each function the disassembler encounters
is parsed individually. Wildcards are allowed in the idioms; for example, the
idiom
\begin{lstlisting}
(push ebp | * | mov esp,ebp)
\end{lstlisting}
%TK: Taken from Rosenblum, find a better example
represents a generic stack frame setup operation.

Step 3: N-gram generation
The disassembled code is parsed again in the same per-function way as above,
this time examining the raw binary data of each function. Each n-gram
(continuous sequence of bytes) of length 3 or 4 is extracted and counted. This
leads to a large amount of data, roughly 64\% of all features in Rosenblum et
al.’s corpus.

Step 4: Generation of the control flow graph (CFG)
This can be done using standard tools such as the open source ParseAPI library
(Paradyn, 2016). The CFG is a directed graph whose nodes and edges represent
the basic blocks and control flow paths respectively of the disassembled
program. See section 2.TK for a more in-depth explanation.

Step 5: Generation of the coloured control flow graph (CCFG) the randomly
collapsed, coloured control flow graph (RCCCFG) and the coloured call graph
(CCG).
Using the CFG, three derivative graphs are created: The coloured CFG (CCFG),
the randomly collapsed, coloured CFG (RCCCFG) and the coloured call graph
(CCG). To generate the CCFG, each node in the CFG is associated with a
fourteen-bit colour depending on the types of instructions contained in the
node’s corresponding block of code (this process is described in detail in
\parencite{rosenblum2011wrote}). The RCCCFG is then generated from the CCFG by collapsing
all nodes in the CCFG with a random neighbor in the natural way:
Fig 2.X TODO: REPLACE THESE IMAGES

The CCG is created from the CFG only including the subset of nodes that contain
a call instruction, with edges between two nodes representing a path between
the corresponding nodes in the original CFG. A colouring function is applied in
the same way as for the CCFG, with the colours depending on whether not the
node calls a library function, and if so, which library function is called:
Fig 2.X TODO: REPLACE THIS IMAGE


Step 6: Generation of feature vectors
For each author in the data set an integral valued feature vector is generated
from the  above steps. Specifically, the feature vector contains:

\begin{itemize}
\item All idioms and n-grams
\item All three-node subgraphs of the of the CCFG
\item All three-node subgraphs of the RCCCFG
\item All three-node subgraphs of the CCG
\item The number of times a library method is called, extracted from the CCG
\end{itemize}

Step 7: Feature selection
This can be seen as a dimensionality reduction of the feature vectors. It is
performed by selecting the features that are ranked best under the mutual
information criteria with respect to the author labels. The optimal number of
features to retain is chosen by training the model with 10-fold cross
validation on the dataset.

\subsubsection{Evaluation}
Rosenblum et al. evaluate their data on both a private data set obtained from
an operating systems course (CS537) at the University of Wisconsin and a subset
of correct solutions from the 2009 and 2010 rounds of the Google Code Jam
(GCJ), specifically the solutions that:

\begin{itemize}
\item Were written in C/C++
\item Could be compiled using GCC 4.5 
\item Were submitted by an author that had submitted correct solutions to at
      least 8 problems in total
\end{itemize}

TK: We only care about 2010 data
In total, the GCJ data set consisted of 2581 binaries (1,747 binaries from
2010) from 284 authors. Using 10-fold cross-validation on a set of 191 authors
from 2010, 51\% accuracy was achieved. For a randomly selected subset of 10
authors, 81\% accuracy was achieved and for a randomly selected subset of 20
authors 77\% accuracy.

\subsection{Alrabaee et al.}
\subsubsection{Approach and evaluation}
Another study by Alrabaee et al. (An Onion approach to Binary code Authorship
Attribution) used an approach they called OBA2 to identify author style in
binary code \parencite{alrabaee2014oba2}. The study refers to the work of
Rosenblum et al. and argues that some of the features used in Rosenblum et al.
do not represent author style features. The result of the study (diagrams)
shows a higher accuracy than \parencite{rosenblum2011wrote}.The data used in
the study was the same as Rosenblum et al. 

\subsection{Caliskan-Islam et al.}
\subsubsection{Approach and evaluation}
Another report published in December 2015 by Caliskan-Islam et al. improves on
the work of Rosenblum. This study has been able to improve author
identification accuracy even further also using data from Google Code Jam and
c/c++ binary code. With a set of 20 authors they are able to find the correct
author with 96\% accuracy. Caliskan-Islam (2015) also tested the effect of
different optimisations and got with 100 authors and no optimisation 78.3\%
accuracy and with level 3 optimisation 60.1\% accuracy. Furthermore, the study
found that it was easier to identify more experienced programmers.

\chapter{Method}
\section{Data and datasets}
We base our analysis on data collected from the 2010 round of Google Code Jam
(GCJ). Specifically, we restrict our analysis to authors that have submitted at
least 8 submissions in C++. We also require that solutions consist of exactly
one file with the extension .cpp which can be compiled and statically linked
with g++ in our test environment (see section 3.2 for details).

Three subsets with 20, 191, and all 413 authors of the GCJ 2010 data set are
used in order to test how the size of training data affects our analysis.
Furthermore, analysis on binaries compiled with the -static flag is
computationally infeasible with large datasets in our test environments. This
is summarised in Table~\ref{tab:datasets-summary}.

\begin{table}[!htb]
    \centering
        \begin{tabular}{ l l l }
        Authors & Subsets & Compiler flags \\ \hline
        413 & 413, 20 &  \lstinline{-O0, -O1, -O2, -O3} \\ 
        191 & 191, 20, 10 &  \lstinline{-O0, -O3} \\ 
        20 & 20, 10 &  \lstinline{-O0 static} 
        \end{tabular}
    \caption{Summary of subsets of the 2010 GCJ dataset used in our analysis}
    \label{tab:datasets-summary}
\end{table}

These dataset is chosen primarily to allow us to easily compare our results
with the existing literature; however, it should be said that the structure of
the GCJ dataset is a good fit for our analysis nonetheless. Because we use an
unsupervised classification algorithm with a large number of features, it is
important to choose features derived from the coding style of the programmer
and not the problem itself. The GCJ dataset facilitates this since all
contestants solve the same set of problems; features derived from the problem
formulation perform poorly as classifiers and can be identified and discarded.
Likewise, features derived from the compilation process itself can also be
identified and discarded due to all solutions being compiled in the same way.

\section{Test environment}
All processing was done on an server running Fedora 23 x64.
Table~\ref{tab:dnf-packages} lists the dnf packages used in our test
environment that are not included in the base installation used:

\begin{table}[!htb]
    \centering
        \begin{tabular}{ l l l }
        Package name & Version & Usage \\ \hline
        gcc-c++ & 5.3.1-2.fc23 & Compiling GCJ submissions \\
        python & 2.7.10-8.fc23 & Scripting, data manipulation \\
        libsvm & 3.20-5.fc23  & Support vector machine library \\
        libsvm-python & 3.20-5.fc23 & Python bindings for libsvm
        % TK: This is incomplete
        \end{tabular}
    \caption{\lstinline{dnf} packages not including in the base Fedora 23 x64
    installation that are used in our analysis}
    \label{tab:dnf-packages}
\end{table}

In a supplement to their 2011 paper, Rosenblum et al. provided the programs
they created for binary feature extraction as open source software. We used
these same programs for our own analysis, albeit with minor changes due to
outdated dependencies\footnote{
    The original feature extraction code can be found at
    \url{http://pages.cs.wisc.edu/~nater/esorics-supp/}. Our modified versions can be
    found at \url{https://github.com/macaullyjames/esorics}, along with installation
    instructions.
}.

\section{Feature extraction}
The techniques presented in \parencite{rosenblum2011wrote} are applied to each
compiled program in the dataset (see section 2.TK for details) so that each
file is associated with a list of string features. A list of all unique
features in the dataset is then generated (figure TK). For each author, we
count the total number of occurrences of each feature in their solution set and
save this data in the LIBSVM format. Each feature is mapped to its index in the
global feature list (figure TK).

\begin{table}[!htb]
    \centering
        \begin{tabular}{ l l l }
        Compiler setting & Unique features & Total features \\ \hline
        \lstinline{-O0} & 765307 & 13204201 \\
        \lstinline{-O3} & 3891494 & 21570182
        \end{tabular}
    \caption{Features in the 413 author dataset}
    \label{tab:413-features}
\end{table}

\begin{table}[!htb]
    \centering
        \begin{tabular}{ l l l }
        Compiler setting & Unique features & Total features \\ \hline
        \lstinline{-O0} & 509908 & 6221646 \\
        \lstinline{-O3} & 2380127 & 10276979
        \end{tabular}
    \caption{Features in the 191 author dataset}
    \label{tab:191-features}
\end{table}

\begin{table}[!htb]
    \centering
        \begin{tabular}{ l l l }
        Compiler setting & Unique features & Total features \\ \hline
        \lstinline{-O0 -static} & 3859605 & 110725073 \\
        \end{tabular}
    \caption{Features in the 20 author dataset}
    \label{tab:20-features}
\end{table}

\section{Feature selection}
The feature extraction process yields a high dimensional dataset with hundreds
of thousands, sometimes millions, of features. In order to speed up the
training of the SVM and avoid overfitting it is desirable to remove features
that provide little or no information about the authors’ programming style. To
achieve this, we first rank the features according to the mutual information
criteria \parencite{guyon2003introduction} and then evaluate a model trained
with varying sizes of subsets of the highest ranking features. The model with
the best predictive power is retained.

\subsection{Ranking using mutual information}
Let $X$ be the multiset of all features generated for a given data set. For all
$x \in X$, let $P(X = x)$ denote the observed frequency of $x$ in $X$, that is,
$P(X = x) = \frac{1}{|X|}$. Likewise, let $Y$ be the set of all author labels
and $P(Y = y)= \frac{1}{|Y|}$ be the observed frequency of the label $y$ in $Y$
and let $P(X=x_i,Y=y)$ denote the joint frequency of $x_i$ and $y$ in the data
set. We rank each unique feature $x_i \in supp X $ using the mutual information
criteria $R(x_i)$ (Guyon, 2003) in the following way:

$$R(x_i)= \sum_{x_i \in X, y \in Y} P(X=x_i,Y=y) \log \frac{P(X = x_i,
Y = y)}{P(X = x_i)P(Y = y)}$$

Intuitively the highly ranked features will be unique to a small set of author
labels and as such more useful in the classification process, whereas the
lowest ranked features can be removed without negatively affecting the
predictive power of the model.  Determining the optimal subset of features
Using the above ranking algorithm we train our model using the 800, 1900, 7000,
8000, 9000, 10000 and 100000 highest ranked features, choosing the subset of
features that yields the best predictive power. We also compare with a model
trained with all features in data sets where this is computationally feasible.
The predictive power of each model is validated through cross-validation, see
section TK for details.

\section{Author classification}
As in the original paper from Rosenblum et al., classification is performed
using a linear support vector machine (SVM) \parencite{rosenblum2011wrote}. See
section 2.TK for details. We use the svm-scale program from the LIBSVM toolkit
\parencite{chang2011libsvm} to scale our feature values (avoids overweighting
frequently occurring features) to the interval [0, 1]:
\begin{lstlisting}
svm-scale -l 0 -u 1 features.dat > features.scale
\end{lstlisting}

We then use the LIBLINEAR SVM implementation \parencite{fan2008liblinear} for
training and evaluating via 10-fold cross validation:
\begin{lstlisting}
train -v 10 features.scale
\end{lstlisting}


\chapter{Results}
This section present the results from our different experiments using 10-fold
cross-validation to obtain the accuracy values. The experiments vary in number
of authors used, total number of features and processing. The purpose is to
show how the accuracy varies in different scenarios.  

\section{Test with 413 authors and different optimisation levels}
This experiment uses a dataset of 413 authors and optimisation levels -O0 to
-O3. Moreover, this experiment examine the accuracy when using different
numbers of top ranked features in the training dataset in an attempt to find a
maximum value. To get more accurate values the average of many test instances
is used. In this case 20 subsets with data from 20 randomly selected authors
from the total of 413 authors were used.

\subsection{With all 413 authors and optimisation level -O0 to -O3}
Figure 4.1. This diagram sums up the results of using the entire set of 413
authors with different optimisation levels and top ranked features.
Optimisation with -O0 gave the highest accuracy 32.53\% using 7000 features.
Optimisation with -O1 gave its highest accuracy 28.60\% using 9000 features.
Optimisation with -O2 gave its highest accuracy 28.60\% using 10000 features.
Optimisation with -O3 gave its highest accuracy 21.63\% using 10000 features. 


\subsection{Subset with 20 authors and optimisation level -O0}
Figure 4.2. This diagram shows the results of binaries that were compiled with
optimisation level -O0 and had a total of 765307 unique features. The x-axis
shows the number of top ranked features used  and the y-axis shows the average
of the results from the 20 subsets used. The highest accuracy average achieved
was 68.80\% with 8000 features. 

Figure 4.3. In this diagram the result of every instance of the 20 instances
using 8000 features and with 20 randomly selected authors is displayed.
Standard deviation is 5.18\%. 

\subsection{Subset with 20 authors and optimisation level -O1}
Figure 4.4. This diagram shows the results of binaries that were compiled with
optimisation level -O1 and had a total of 2141558 unique features. The x-axis
shows the number of top ranked features used and the y-axis shows the average
of the results from the 20 subsets used. Highest accuracy average achieved was
65.42\% with 7000 features. 


Figure 4.5. In this diagram the result of every instance of the 20 instances
using 7000 features and with 20 randomly selected authors is displayed.
Standard deviation is 6.86\%.

\subsection{Subset with 20 authors and optimisation level -O2}
Figure 4.6. This diagram shows the results of binaries that were compiled with
optimisation level -O2 and had a total of 2593340 unique features. The x-axis
shows the number of top ranked features used and the y-axis shows the average
of the results from the 20 subsets used. Highest accuracy average achieved was
64.43\% with 9000 features.

Figure 4.7. In this diagram the result of every instance of the 20 instances
using 9000 features and with 20 randomly selected authors is displayed.
Standard deviation is 6.99\%.

\subsection{Subset with 20 authors and optimisation level -O3}
Figure 4.8. This diagram shows the results of binaries that were compiled with
optimisation level -O3 and had a total of 3891394 unique features. The x-axis
shows the number of top ranked features used and the y-axis shows the average
of the results from the 20 subsets used. Highest accuracy average achieved was
58.66\% with 9000 features.

Figure 4.9. In this diagram the result of every instance of the 20 instances
using 9000 features and with 20 randomly selected authors is displayed.
Standard deviation is 5.73\%.

\subsection{Summary}
Figure 4.X. Comparison of results from the different optimisations levels using
staples.  

Figure 4.X. Comparison of results from the different optimisations levels using
lines.  

\section{Test with 191 authors and different optimisation levels}
This experiment uses a dataset of 191 authors and optimisation levels -O0 and
-O3. It shows how fewer authors affect the results and is the same number of
authors used in Rosenblum et al. However, the exact set of authors may be
different and the number of files is not the same. Moreover, this experiment
examine the accuracy when using different numbers of top ranked features in the
training dataset in an attempt to find a maximum value. To get more accurate
values the average of many test instances is used. In this case 20 subsets with
data from 20 randomly selected authors from the total of 191 authors were used.
In addition, tests with 10 authors instead of 20 was also used.  

\subsection{With all 191 authors and optimisation level -O0 and -O3}
Figure 4.X: This diagram sums up the results of using the entire set of 191
authors with optimisation levels -O0, -O3 and different numbers of top ranked
features. Optimisation with -O0 gave the highest accuracy 41.80\% using 7000
features. Optimisation with -O3 gave its highest accuracy 29.61\% using 10000
features. 

\subsection{Subset with 20 authors and optimisation level -O0}
Figure 4.X. This diagram shows the results of binaries that were compiled with
optimisation level -O0 and had a total of 509908 unique features. The x-axis
shows the number of top ranked features used  and the y-axis shows the average
of the results from the 20 subsets used. The highest accuracy average achieved
was 71.22\% with 8000 features.

Figure 4.X.  In this diagram the result of every instance of the 20 instances
using 8000 features and with 20 randomly selected authors is displayed.
Standard deviation is 6.18\%.

\subsection{Subset with 20 authors and optimisation level -O3}
Figure 4.X. This diagram shows the results of binaries that were compiled with
optimisation level -O3 and had a total of 2380127 unique features. The x-axis
shows the number of top ranked features used and the y-axis shows the average
of the results from the 20 subsets used. Highest accuracy average achieved was
59.79\% with 9000 features.

Figure 4.X.  In this diagram the result of every instance of the 20 instances
using 9000 features and with 20 randomly selected authors is displayed.
Standard deviation is 5.66\%.

\subsection{Subset with 10 authors and optimisation level -O0}
Figure 4.X.  This diagram shows the results of binaries that were compiled
with optimisation level -O0 and had a total of 509908 unique features. The
x-axis shows the number of top ranked features used and the y-axis shows the
average of the results from the 20 subsets used, this time with 10 authors.
Highest accuracy average achieved was 81.58\% with 8000 features.

Figure 4.X. In this diagram the result of every instance of the 0 instances
using 8000 features and with 10 randomly selected authors is displayed.
Standard deviation is 7.09\%.

\subsection{Subset with 10 authors and optimisation level -O3}
Figure 4.X. This diagram shows the results of binaries that were compiled with
optimisation level -O3 and had a total of 2380127 unique features. The x-axis
shows the number of top ranked features used and the y-axis shows the average
of the results from the 20 subsets used, this time with 10 authors. Highest
accuracy average achieved was 73.22\% with 10000 features.

Figure 4.X. In this diagram the result of every instance of the 20 instances
using 10000 features and with 10 randomly selected authors is displayed.
Standard deviation is 9.24\%.

\subsection{Summary}
Figure 4.X. Comparison of results from the different optimisations levels and
20 authors.

Figure 4.X. Comparison of results from the different optimisations levels and
10 authors.

Figure 4.X. Comparison of results with different number of authors using
staples.

Figure 4.X. Comparison of results with different number of authors using
lines.

\section{Test with 20 authors and static compilation}
This experiment uses a dataset of 20 authors and without  optimisation. It
shows how the use of static affect the results. Moreover, this experiment
examine the accuracy when using different numbers of top ranked features in the
training dataset in an attempt to find a maximum value.  To get more accurate
values the average of many test instances is used. In this case 20 subsets with
data from 10 randomly selected authors from the total of 20 authors were used.
In addition, tests with each data file with 20 authors was used; however, no
average could be used. Tests with all features were not possible or took too
long time to obtain in this case.   

\subsection{With all 20 authors and static (optimisation level -O0)}
Figure 4.X. This diagram shows the results of binaries that were compiled with
static and had a total of 3859605 unique features. The x-axis shows the number
of top ranked features used and the y-axis shows the the results from a single
set with 20 authors. Highest accuracy achieved was 28.65\% with 9000 features.

\subsection{Subset with 10 authors and static (optimisation level -O0)}
Figure 4.X. This diagram shows the results of binaries that were compiled with
static and had a total of 3859605 unique features. The x-axis shows the number
of top ranked features used and the y-axis shows the average of the results
from the 20 subsets used, this time with 10 authors. Highest accuracy average
achieved was 45.54\% with 8000 features.

Figure 4.X. In this diagram the result of every instance of the 20 instances
using 8000 features and with 10 randomly selected authors is displayed.
Standard deviation is 6.62\%.

\subsection{Summary}
Figure 4.X. Comparison of results with different number of authors using
staples.

Figure 4.X. Comparison of results with different number of authors using lines.


\chapter{Discussion}
\section{Method discussion}
When attempting to implement the method used in Rosenblum et al. it was not
without difficulties. To be able to use the source code provided by Rosenblum
et al. we had to make some adjustments to the source code, updating it to make
it work on a more modern system. As far as we can tell it has not caused any
errors.

The source code provided by Rosenblum et al. was only for feature extraction.
We assume that possible inconsistencies with the method of Rosenblum et al.
come from the process of converting output data from the feature extraction to
data with LIBSVM format needed to be able to use LIBLINEAR.

We notice that we need more features than Rosenblum et al. to achieve high
accuracy; thus, our implementation of feature selection may differ from that of
Rosenblum et al. 

\section{Results analysis}
The experiments conducted show that optimisation reduces the accuracy. This is
similar to what \parencite{caliskan2015coding} found in their experiments. As
previously mentioned, they got with 100 authors and no optimisation 78.3\%
accuracy and with level 3 optimisation 60.1\% accuracy.

When optimisation is used to compile the binaries there tend to be a larger
amount of features extracted. Optimisation with -O3 gave 3891494 unique
features in the case with 413 authors compared to 736307 features when -O0 was
used. That is more than five times as many features. For optimisation -O3  the
highest accuracy with 20 authors was 58.66\% compared to 68.80\% when no
optimisation was used (see Figure 4.2. and Figure 4.8.), a 10.14\% difference).
Furthermore, optimisation with -O1 and -O2 gave only a small reduction in
accuracy. The results are still much higher than the random chance 5\% of
guessing the author . 

Experiments with subsets of 20 authors from the set of 191 authors shows
similar results. When no optimisation was used the highest accuracy was 71.22
\% compared to 59.79\% when -O3 was used (see Figure 4.X. and  Figure 4.X. ).
In this case the difference is 11.43\%.

It is also possible to see that the accuracy is higher when fewer authors is
used which is not that strange, random chance of guessing the correct author is
also higher 10\%. If the case with 20 authors and no optimisation from the set
of 191 is compared with that of 10 authors (see Figure 4.X.) the highest
accuracy with 10 authors and was 81.58\% compared to 71.22\% with 20 authors.
The difference is 10.36\%.

When using static with only 20 authors the unique number of features was
3859605 and the total number of features was 110725073. In contrast, even
though using more authors,  the number of unique features from the set of 191
authors was 509908 and the total number of features was only 6221646.The
accuracy achieved in the case using static linking and 10 authors was 45.54\%
compare to 81.58\%. Thus, an important factor affecting the accuracy seems to
be the number of features. 

Even though we couldn’t replicate Rosenblum’s et al. 2011 experiment and data
exactly our experiments showed accuracy levels not far from theirs. Using 191
authors and 20 authors \parencite{rosenblum2011wrote} got approximately 77\% while
our experiment using 191 authors; however, with a different number of files and
probably not the exact set of 191 authors our experiment gave 71\%.     

With no optimisation and using all 191 authors and 7000 features an accuracy of
41.8\% was achieved (see Figure 4.23). It is much better than random chance
0.5\%. \parencite{rosenblum2011wrote} achieved 51\% on the full set with 191
authors using only 1900 features. Not only did they achieve better results,
they also manage to do this using fewer features. Which could indicate that our
implementation of feature selection differ from that of Rosenblum et al.

\section{Future research}
There are more techniques one could use when processing the source code or the
binaries that could be interesting to investigate further. It would be
interesting to test how different packers affect the accuracy. Moreover, it
would be interesting to see how this method would work in more practical
situations. Is it possible to identify teams using these techniques? What if
the team’s members change frequently? What if two teams enforce the same style
guide? In real situations the code could be processed using different methods.
What would happen if the binaries were compiled with a mix of different
optimisation levels and static?  

Most of the previous reports have focus on code from the Google Code Jam
programming competition. More research on other datasets with larger files
could be interesting to examine.

With the purpose of testing the accuracy values obtained it could also be
interesting to use different machine learning approaches. Possible try another
program than LIBLINEAR for the cross-validation.

\chapter{Conclusion}
This thesis attempted to verify the authorship attribution method created by
\parencite{rosenblum2011wrote}. Our results tend to be somewhat lower than that
of Rosenblum et al. and need more features to give high accuracy; nevertheless,
the method seems to be working. Some possible explanations concerning the
differences in accuracy achieved could be because of difficulties in
implementing Rosenblum et al. method exactly. Furthermore, even though working
with data from Google Code Jam 2010 the exact dataset that Rosenblum et al.
used could not be replicated and therefore cause differences in results. 

Optimisation seems not be enough to make authors anonymous; however, it reduces
the accuracy somewhat. Adding static causes a bigger drop in accuracy. The
accuracy achieved in the case using static and 10 authors was 45.54\% compare
to 81.58\% without. Perhaps not enough to ensure anonymity; nevertheless,
static gives a significantly lower accuracy. The data extracted shows that
optimisation and static linking results in more features that may be the cause
of the reduction in accuracy.

\printbibheading
\printbibliography

\end{document}
