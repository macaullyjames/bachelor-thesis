\documentclass[a4paper,11pt]{kth-mag}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{modifications}
\usepackage{listings}
\usepackage{hyperref}
\title{Anti-analysis techniques to preserve programmer anonymity in binary
executables}

\subtitle{
    DEGREE PROJECT IN COMPUTER SCIENCE, FIRST LEVEL \\
    STOCKHOLM, SWEDEN 2016
}
\author{Johan Wikström \\ Macaully Muir}
\date{April 2016}
\blurb{
    Supervisor: Michael Schliephake \\
    Examiner: Örjan Ekeberg
}
\trita{TRITA TK: xxx yyyy-nn}
\begin{document}
\frontmatter
\pagestyle{empty}
\removepagenumbers
\maketitle
\selectlanguage{english}
\begin{abstract}
Previous research has shown that it is possible to achieve authorship
attribution with high accuracy using binary files. The idea behind authorship
attribution for binaries is that authors’ unique style will survive the
compilation process making it possible to identify the author.    

The aim of this report is to investigate different ways to reduce the accuracy
and see how the processing of binaries affects the author prediction accuracy.

This work will build on the report of Rosenblum et al. and implement the method
they used. The method uses  a machine learning approach to predict the author
of the binary. The data used for prediction consists of features extracted from
the binaries. Similar to Rosenblum et al. (2011) binary files used in this
thesis come from the Google Code Jam programming competition.

Results from this study indicate that optimisation of code reduces the accuracy
achieved; however, is not enough to ensure anonymity. Using static linking
results in a more significant drop in accuracy. The data extracted shows that
optimisation and static linking results in more features that may be the cause
of the reduction in accuracy.
\end{abstract}
\clearpage
\begin{foreignabstract}{swedish}
I tidigare studier har det visat sig möjligt att med hög noggrannhet
identifiera författaren till binära filer. Idéen är att författare till kod
lämnar karaktäristiska drag i koden som går att känna igen även efter
kompilering. 

Syftet med denna rapport är att utforska olika tekniker för att minska
säkerheten då man vill bestämma vem som är författaren till binära filer och
undersöka hur olika bearbetningar av koden påverkar noggrannheten.     

Denna rapport bygger vidare på rapport av Rosenblum et al. och dess metod. En
maskininlärningsmetod implementerats för att identifiera vem som är författaren
till en binär fil. Attribut extraheras från binärfilerna som sedan kan används
som data för maskininlärning. Likt Rosenblum et al. används data hämtat från
programmeringstävlingen Google Code Jam. 

Erhållna resultat tyder på att optimering försämrar noggrannheten men är dock
inte tillräckligt för att garantera anonymitet. Användning av statisk länkning
resulterar i en kraftigare sänkning. Man kan se att optimering och statisk
länkning resulterar i fler attribut vilket kan vara orsaken till att det blir
svårare att avgöra vem som är författaren.
\end{foreignabstract}
\clearpage
\tableofcontents*
\mainmatter
\pagestyle{newchap}

\chapter{Introduction}
Is it possible to extract information about programming style from compiled
binary programs? Can such information be used to identify the author of a given
program? In 2011 Rosenblum et al. showed that the answer to both these
questions is “yes” under certain conditions (Rosenblum, 2011); from a pool of
191 Google Code Jam (GCJ) contestants with eight or more submitted solutions,
authorship identification from a set of 10 authors could be performed with 81\%
accuracy using a supervised support vector machine classifier. In 2015
Caliskan-Islam et al. proposed an alternate method that could achieve 96\%
accuracy from a pool of 20 contestants (Caliskan-Islam, 2015) under similar
conditions.

However, it is not clear whether the above results could be replicated with
other datasets from meaningful real-world scenarios. Nevertheless, they do
raise questions about the ability of programmers to write and distribute
software anonymously and it is therefore desirable to find methods that prevent
de-anonymisation with this kind of analysis. Using the methods of (Rosenblum
2011), this report aims to investigate how changes to the build process can be
used to reduce the accuracy of author classification on the same Google Code
Jam dataset.

\section{Problem definition}
In short, this thesis aims to:
\begin{itemize}
\item Validate the results of (Rosenblum, 2011) on the 2010 Google Code Jam dataset
\item Investigate how g++’s optimisation flags (O0-O3) affect the accuracy of the
      author classification algorithm on the 2010 GCJ dataset
\item Investigate how statically linking the standard library affects the accuracy of
      the author classification algorithm on the 2010 GCJ dataset.
\end{itemize}

\section{Motivation}
There are a number of legitimate reasons why the author of a piece of software
would want to remain anonymous. It could be that their association with the
software could put them in danger (for example, a programmer that writes
anti-surveillance software in an oppressive state). It could also be the case
that the programmer simply does not want to be publicly associated with the
software such as in the cases of TrueCrypt, Bitcoin etc. (TK: source). It is
therefore important to investigate the robustness of these author
classification algorithms and develop tools and procedures that ensure can help
ensure the anonymity of the programmer.

\section{Scope}
This report will only implement the method of Rosenblum et al. by using
available source code when possible and by implement remaining part ourselves
trying to be consistent with the original implementation.  Furthermore, we
limit our analysis to a subset of C++ solutions from the 2010 rounds of Google
Code Jam. This allows us to verify the results in the literature (Rosenblum,
2011) and provides a meaningful data set to work with. This method assume that
a program has exactly one author. However, this is often not true since many
programs are developed in teams.

\chapter{Background}
This section begins with an overview of the field. We then introduce the
technical concepts required to understand the rest of this thesis, and finally
present the state-of-the-art in the field of authorship attribution. Focus will
be on deriving authorship from binary programs; however, some related works
will be mentioned.

\section{Overview of the field}
Computer forensics is a branch where the goal is to to analyse digital data to
obtain information, often in relation to computer crime (Reith et al., 2002).
One application is author attribution. In recent years machine learning
techniques have been used with great success to identify counterfeited
integrated circuits (Huang, 2013), distinguishing different kinds of brain
tumours (Zacharaki, 2009) and the automated detection of software bugs (Aleem,
2015), to name a few examples.

The first attempt at using statistical methods to de-anonymise authors of
programs from their coding style was performed by Oman et al. (Oman, 1989) who
classified programmers using a hand-picked, language-specific feature set. In
2006, Frantzeskou et al. proposed a method using the analysis of byte level
n-grams (continuous sequences of n bytes) present in the source code which
greatly improved upon previous methods, along with being language-agnostic
(Frantzeskou, 2006). In 2015, Caliskan-Islam et al. presented a method based on
features of the abstract syntax trees (ASTs) of C/C++ programs that
significantly improved the state-of-the-art again (Caliskan-Islam, 2015).
 
The above methods all assume that de-anonymisation is to be performed on the
source code of a program. However, there are many contexts where
de-anonymisation is desirable where access to the source code is either
impractical or entirely impossible, for example during malware analysis. In
2011 Rosenblum et al. showed that this is is feasible in certain conditions
using machine learning techniques on the stylistic features of source code that
survive the compilation process (Rosenblum, 2011). Specifically, their features
were derived from the control flow graph (CFG) of the compiled binary and byte
level n-grams. In 2014 Alrabaee et al. improved upon the accuracy of those
techniques and proposed features that were more closely related to programming
style (Alrabaee, 2014). In 2015 Caliskan-Islam et al. further improved the
state of the art using disassemblers and decompilers, and applying their
research in de-anonymising programmers from source code to the ASTs of the
decompiled binaries (Caliskan-Islam, 2015).

\section{Technical Background}
This section includes the technical details necessary for understanding the rest of this thesis. We start with a brief overview of the compilation process and the static analysis, techniques used to extract features from binary executables and a short explanation of support vector machines (SVMs) and their uses. 

\subsection{The compilation process}
 Compilation is the process of transforming source code into executable programs. It is beyond the scope of this thesis to give a complete overview of the inner workings of compilers, so we work under the simplification that compilation is a two step process: assembly code generation and machine code generation.

[Figure]

As illustrated in figure 2.TK, stylistic elements present in source code are
not easily discernible from the resulting binary code. This can be formalised
with the observation that compilation is in general non-reversible; there are
an infinite number of source code programs that would generate a given binary
executable. As such the compilation process suffers from information loss,
since information contained within the source code may not be present in the
resulting binary. Figure 2.TK shows two different source code representations
of the same binary program.

[Figure]

However, it is hard to say exactly what information is preserved in the
compilation process. The compiler guarantees that the semantics of the source
code be preserved in the resulting machine code but its algorithms and data
structures may not. Figure 2.TK shows how gcc optimises the calculation of an
arithmetic sum, replacing the loop with the constant-time formula. The results
of (Rosenblum, 2011), (Caliskan-Islam, 2015), and (Alrabaee et al., 2014) are
novel in this regard as they show that stylistic features present in source
code do in fact consistently survive the compilation process.

[Figure]
\subsection{Static analysis of compiled binaries}
Static binary analysis is the process of understanding how a binary program
will behave without executing it. This can involve any number of techniques
but is often used in the context of reverse engineering, that is, the goal of
the analysis is to understand not only the behaviour but also the
implementation of the program. It is assumed that the source code of the
program is not accessible. In this section we present two methods of static
analysis, disassembling and control flow analysis, that are used in our
analysis of the Google Code Jam submissions.

Disassembling is the task of transforming binary code into the equivalent
assembly code and is performed by a disassembler. In general this is a
non-trivial task, especially on machine code for complex instruction sets such
as x86 (Wartell, 2011). Moreover, a number of techniques can be employed to
purposely thwart commonly used disassembly algorithms, making the process much
harder (Linn, 2003). For the purpose of our analysis however this is not an
issue, as we have found that the compiled Google Code Jam solutions in our data
set can in fact be automatically disassembled. Figure 2.TK shows a simple
assembly routine, its equivalent machine code after compilation and the
resulting assembly code after disassembly.

[figure]

Control flow analysis is performed on assembly code produced by a disassembler.
The goal is to partition the code into so-called “basic blocks”: sequences of
instructions with exactly one entry point and exit point with respect to
program flow (Allen 1970). This structure is captured in the control flow graph
(CFG), a directed graph with the basic blocks as nodes and control flow paths
as edges. Figure 2.TK illustrates the basic blocks of a simple assembly code
routine.

[figure]

The CFG provides a structured view of the binary at a higher level than
individual instructions. This can be used to identify high-level language
constructs such as loops and functions (Cifuentes, 1993) and as such is used by
decompilers such as the Hex-Rays decompiler (Hex-Rays, 2016), used by
Caliskan-Islam et al. in their paper improving on the work of Rosenblum et al
(Caliskan-Islam 2015). Rosenblum et al. also use the CFG for feature
extraction, but do not go as far as to decompile the binaries (Rosenblum 2011).

\subsection{Support vector machines}
Support vector machines (SVMs) belong to a group of supervised learning methods
and are based on statistical learning theory. SVMs have been around since the
1960s and work well on for example text and image recognition. The learning
algorithm is given data and learns from that to identify patterns or classify
new data. There is a balance in how much data the algorithm should have. Using
too much information can lead to “overfitting” and the algorithm will perform
poorer for new data (Weston, n.d.). The goal is to maximize the distance
between data points in the different classes (Awad, 2004).

\subsection{Cross-validation}
Cross-validation is a common method used when testing predictions. It is a
model used to analyze how well a predictive model will work in practice. When
using 10-fold cross-validation the data is randomly divided into 10 equally
sized sets, 9 of the sets is used for training and the remaining set it used
for the testing. This is repeated for each one of the sets, finally resulting
in an average measure of accuracy in our case (Hsu, 2003).        

\section{State-of-the-art analysis}
As previously discussed, there are as far as we are aware of exactly three
papers presenting novel techniques to de-anonymise programmers from compiled
binary programs: (Rosenblum, 2011), (Alrabee, 2014) and (Caliskan-Islam et al.,
2015). In this section we discuss each approach. The  focus will be on the
techniques used by Rosenblum et al.

\subsection{Rosenblum et al.}
\subsubsection{General approach}
Step 1: Binary disassembly
As a first step, the compiled binary is disassembled into human-readable
assembly code. It is implicitly assumed that this can always be performed, but
it is important to note that this is not necessarily the case. Malware, for
example, is often obfuscated in ways that make automated disassembly difficult
to perform (Branco, 2012). Rosenblum’s approach will not work if the binary
cannot be disassembled.

Step 2: Idiom generation
The disassembled code is parsed and all sequences of 1-3 instructions are
extracted and counted. Specifically, each function the disassembler encounters
is parsed individually. Wildcards are allowed in the idioms; for example, the
idiom
\begin{lstlisting}
(push ebp | * | mov esp,ebp)
\end{lstlisting}
%TK: Taken from Rosenblum, find a better example
represents a generic stack frame setup operation.

Step 3: N-gram generation
The disassembled code is parsed again in the same per-function way as above,
this time examining the raw binary data of each function. Each n-gram
(continuous sequence of bytes) of length 3 or 4 is extracted and counted. This
leads to a large amount of data, roughly 64\% of all features in Rosenblum et
al.’s corpus.

Step 4: Generation of the control flow graph (CFG)
This can be done using standard tools such as the open source ParseAPI library
(Paradyn, 2016). The CFG is a directed graph whose nodes and edges represent
the basic blocks and control flow paths respectively of the disassembled
program. See section 2.TK for a more in-depth explanation.

Step 5: Generation of the coloured control flow graph (CCFG) the randomly
collapsed, coloured control flow graph (RCCCFG) and the coloured call graph
(CCG).
Using the CFG, three derivative graphs are created: The coloured CFG (CCFG),
the randomly collapsed, coloured CFG (RCCCFG) and the coloured call graph
(CCG). To generate the CCFG, each node in the CFG is associated with a
fourteen-bit colour depending on the types of instructions contained in the
node’s corresponding block of code (this process is described in detail in
(Rosenblum, 2011)). The RCCCFG is then generated from the CCFG by collapsing
all nodes in the CCFG with a random neighbor in the natural way:
Fig 2.X TODO: REPLACE THESE IMAGES

The CCG is created from the CFG only including the subset of nodes that contain
a call instruction, with edges between two nodes representing a path between
the corresponding nodes in the original CFG. A colouring function is applied in
the same way as for the CCFG, with the colours depending on whether not the
node calls a library function, and if so, which library function is called:
Fig 2.X TODO: REPLACE THIS IMAGE


Step 6: Generation of feature vectors
For each author in the data set an integral valued feature vector is generated
from the  above steps. Specifically, the feature vector contains:

\begin{itemize}
\item All idioms and n-grams
\item All three-node subgraphs of the of the CCFG
\item All three-node subgraphs of the RCCCFG
\item All three-node subgraphs of the CCG
\item The number of times a library method is called, extracted from the CCG
\end{itemize}

Step 7: Feature selection
This can be seen as a dimensionality reduction of the feature vectors. It is
performed by selecting the features that are ranked best under the mutual
information criteria with respect to the author labels. The optimal number of
features to retain is chosen by training the model with 10-fold cross
validation on the dataset.

\subsubsection{Evaluation}
Rosenblum et al. evaluate their data on both a private data set obtained from
an operating systems course (CS537) at the University of Wisconsin and a subset
of correct solutions from the 2009 and 2010 rounds of the Google Code Jam
(GCJ), specifically the solutions that:

\begin{itemize}
\item Were written in C/C++
\item Could be compiled using GCC 4.5 
\item Were submitted by an author that had submitted correct solutions to at
      least 8 problems in total
\end{itemize}

TK: We only care about 2010 data
In total, the GCJ data set consisted of 2581 binaries (1,747 binaries from
2010) from 284 authors. Using 10-fold cross-validation on a set of 191 authors
from 2010, 51\% accuracy was achieved. For a randomly selected subset of 10
authors, 81\% accuracy was achieved and for a randomly selected subset of 20
authors 77\% accuracy.

\subsection{Alrabee et al.}
\subsubsection{Approach and evaluation}
Another study by Alrabaee et al. (An Onion approach to Binary code Authorship
Attribution) used an approach they called OBA2 to identify author style in
binary code (Alrabaee et al., 2014). The study refers to the work of Rosenblum
et al. and argues that some of the features used in Rosenblum et al. do not
represent author style features. The result of the study (diagrams) shows a
higher accuracy than (Rosenblum 2011).The data used in the study was the same
as Rosenblum et al. 

\subsection{Caliskan-Islam et al.}
\subsubsection{Approach and evaluation}
Another report published in December 2015 by Caliskan-Islam et al. improves on
the work of Rosenblum (2011). This study has been able to improve author
identification accuracy even further also using data from Google Code Jam and
c/c++ binary code. With a set of 20 authors they are able to find the correct
author with 96\% accuracy. Caliskan-Islam (2015) also tested the effect of
different optimisations and got with 100 authors and no optimisation 78.3\%
accuracy and with level 3 optimisation 60.1\% accuracy. Furthermore, the study
found that it was easier to identify more experienced programmers.

\chapter{Method}
\section{Data and datasets}
We base our analysis on data collected from the 2010 round of Google Code Jam
(GCJ). Specifically, we restrict our analysis to authors that have submitted at
least 8 submissions in C++. We also require that solutions consist of exactly
one file with the extension .cpp which can be compiled and statically linked
with g++ in our test environment (see section 3.2 for details).

Three subsets with 20, 191, and all 413 authors of the GCJ 2010 data set are
used in order to test how the size of training data affects our analysis.
Furthermore, analysis on binaries compiled with the -static flag is
computationally infeasible with large datasets in our test environments. This
is summarised in figure 3.TK.

These dataset is chosen primarily to allow us to easily compare our results
with the existing literature; however, it should be said that the structure of
the GCJ dataset is a good fit for our analysis nonetheless. Because we use an
unsupervised classification algorithm with a large number of features, it is
important to choose features derived from the coding style of the programmer
and not the problem itself. The GCJ dataset facilitates this since all
contestants solve the same set of problems; features derived from the problem
formulation perform poorly as classifiers and can be identified and discarded.
Likewise, features derived from the compilation process itself can also be
identified and discarded due to all solutions being compiled in the same way.

\section{Test environment}
All processing was done on an server running Fedora 23 x64. The following dnf
packages were used at some point during our analysis:

In a supplement to their 2011 paper, Rosenblum et al. provided the programs
they created for binary feature extraction as open source software. We used
these same programs for our own analysis, albeit with minor changes due to
outdated dependencies\footnote{
    The original feature extraction code can be found at
    \url{http://pages.cs.wisc.edu/~nater/esorics-supp/}. Our modified versions can be
    found at \url{https://github.com/macaullyjames/esorics}, along with installation
    instructions.
}.

\section{Feature selection}
The feature extraction process yields a high dimensional dataset with hundreds
of thousands, sometimes millions, of features. In order to speed up the
training of the SVM and avoid overfitting it is desirable to remove features
that provide little or no information about the authors’ programming style. To
achieve this, we first rank the features according to the mutual information
criteria (Guyon, 2003) and then evaluate a model trained with varying sizes of
subsets of the highest ranking features. The model with the best predictive
power is retained.

\subsection{Ranking using mutual information}
Let $X$ be the multiset of all features generated for a given data set. For all
$x \in X$, let $P(X = x)$ denote the observed frequency of $x$ in $X$, that is,
$P(X = x) = \frac{1}{|X|}$. Likewise, let $Y$ be the set of all author labels
and $P(Y = y)= \frac{1}{|Y|}$ be the observed frequency of the label $y$ in $Y$
and let $P(X=x_i,Y=y)$ denote the joint frequency of $x_i$ and $y$ in the data
set. We rank each unique feature $x_i \in supp X $ using the mutual information
criteria $R(x_i)$ (Guyon, 2003) in the following way:

$$R(x_i)= \sum_{x_i \in X, y \in Y} P(X=x_i,Y=y) \log \frac{P(X = x_i,
Y = y)}{P(X = x_i)P(Y = y)}$$

Intuitively the highly ranked features will be unique to a small set of author
labels and as such more useful in the classification process, whereas the
lowest ranked features can be removed without negatively affecting the
predictive power of the model.  Determining the optimal subset of features
Using the above ranking algorithm we train our model using the 800, 1900, 7000,
8000, 9000, 10000 and 100000 highest ranked features, choosing the subset of
features that yields the best predictive power. We also compare with a model
trained with all features in data sets where this is computationally feasible.
The predictive power of each model is validated through cross-validation, see
section TK for details.

\section{Author classification}
As in the original paper from Rosenblum et al., classification is performed
using a linear support vector machine (SVM) (Rosenblum, 2011a). See section
2.TK for details. We use the svm-scale program from the LIBSVM toolkit (Chang,
2011) to scale our feature values (avoids overweighting frequently occurring
features) to the interval [0, 1]:
\begin{lstlisting}
svm-scale -l 0 -u 1 features.dat > features.scale
\end{lstlisting}

We then use the LIBLINEAR SVM implementation (Chang 2008) for training and
evaluating via 10-fold cross validation:
\begin{lstlisting}
train -v 10 features.scale
\end{lstlisting}


\end{document}
