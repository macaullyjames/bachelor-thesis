\documentclass[a4paper,11pt]{kth-mag}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{modifications}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[backend=bibtex,style=authoryear]{biblatex}
\addbibresource{references.bib}

\title{Anti-analysis techniques to preserve programmer anonymity in binary
executables}

\subtitle{
    DEGREE PROJECT IN COMPUTER SCIENCE, FIRST LEVEL \\
    STOCKHOLM, SWEDEN 2016
}
\author{Johan Wikström \\ Macaully Muir}
\date{April 2016}
\blurb{
    Supervisor: Michael Schliephake \\
    Examiner: Örjan Ekeberg
}
\trita{TRITA TK: xxx yyyy-nn}
\begin{document}
\frontmatter
\pagestyle{empty}
\removepagenumbers
\maketitle
\selectlanguage{english}
\begin{abstract}
Previous research has shown that it is possible to achieve authorship
attribution with high accuracy using binary files. The idea behind authorship
attribution for binaries is that authors’ unique style will survive the
compilation process making it possible to identify the author.    

The aim of this report is to investigate different ways to reduce the accuracy
and see how the processing of binaries affects the author prediction accuracy.

This work will build on the report of Rosenblum et al. and implement the method
they used. The method uses a machine learning approach to predict the author
of the binary. The data used for prediction consists of features extracted from
the binaries. Similar to \parencite{rosenblum2011wrote} binary files used in this
thesis come from the Google Code Jam programming competition.

Results from this study indicate that optimisation of code reduces the accuracy
achieved; however, is not enough to ensure anonymity. Using static linking
results in a more significant drop in accuracy. The data extracted shows that
optimisation and static linking results in more features that may be the cause
of the reduction in accuracy.
\end{abstract}
\clearpage
\begin{foreignabstract}{swedish}
I tidigare studier har det visat sig möjligt att med hög noggrannhet
identifiera författaren till binära filer. Idéen är att författare till kod
lämnar karaktäristiska drag i koden som går att känna igen även efter
kompilering. 

Syftet med denna rapport är att utforska olika tekniker för att minska
säkerheten då man vill bestämma vem som är författaren till binära filer och
undersöka hur olika bearbetningar av koden påverkar noggrannheten.     

Denna rapport bygger vidare på rapport av \parencite{rosenblum2011wrote}
och dess metod. En maskininlärningsmetod implementerats för
att identifiera vem som är författaren till en binär fil. Attribut extraheras
från binärfilerna som sedan kan används som data för maskininlärning. Likt
Rosenblum et al. används data hämtat från programmeringstävlingen Google Code
Jam. 

Erhållna resultat tyder på att optimering försämrar noggrannheten men är dock
inte tillräckligt för att garantera anonymitet. Användning av statisk länkning
resulterar i en kraftigare sänkning. Man kan se att optimering och statisk
länkning resulterar i fler attribut vilket kan vara orsaken till att det blir
svårare att avgöra vem som är författaren.
\end{foreignabstract}
\clearpage
\tableofcontents*
\mainmatter
\pagestyle{newchap}

\chapter{Introduction}
Is it possible to extract information about programming style from compiled
binary programs? Can such information be used to identify the author of a given
program? In 2011 Rosenblum et al. showed that the answer to both these
questions is “yes” under certain conditions \parencite{rosenblum2011wrote};
from a pool of 191 Google Code Jam (GCJ) contestants with eight or more
submitted solutions, authorship identification from a subset of 10 authors could
be performed with 81\% accuracy using a supervised support vector machine
classifier. In 2015 Caliskan-Islam et al. proposed an alternate method that
could achieve 96\% accuracy from a pool of 20 contestants
\parencite{caliskan2015coding} under similar conditions.

However, it is not clear whether the above results could be replicated with
other datasets from meaningful real-world scenarios. Nevertheless, they do
raise questions about the ability of programmers to write and distribute
software anonymously and it is therefore desirable to find methods that prevent
de-anonymisation with this kind of analysis. Using the methods of
\parencite{rosenblum2011wrote}, this report aims to investigate how changes to
the build process can be used to reduce the accuracy of author classification
on the same Google Code Jam dataset.

\section{Problem definition} In short, this thesis aims to:
\begin{itemize}
\item Validate the results of \parencite{rosenblum2011wrote} on the 2010 Google
      Code Jam dataset
\item Investigate how g++’s optimisation flags (O0-O3) affect the accuracy of
      the author classification algorithm on the 2010 GCJ dataset
\item Investigate how statically linking the standard library affects the
      accuracy of the author classification algorithm on the 2010 GCJ dataset.
\end{itemize}

\section{Motivation}
There are a number of legitimate reasons why the author of a piece of software
would want to remain anonymous. It could be that their association with the
software could put them in danger (for example, a programmer that writes
anti-surveillance software in an oppressive state). It could also be the case
that the programmer simply does not want to be publicly associated with the
software such as in the cases of TrueCrypt, Bitcoin etc. (TK: source). It is
therefore important to investigate the robustness of these author
classification algorithms and develop tools and procedures that ensure can help
ensure the anonymity of the programmer.

\section{Scope}
This report will only implement the method of Rosenblum et al. by using
available source code when possible and by implement remaining part ourselves
trying to be consistent with the original implementation. Furthermore, the report 
will only analyse a subset of C++ solutions from the 2010 rounds of Google
Code Jam. This makes it possible to verify the results in the literature
\parencite{rosenblum2011wrote} and provides a meaningful data set to work with.
The method assume that a program has exactly one author. However, in real cases this is
often not true since many programs are developed in teams.

\chapter{Background}
This section begins with an overview of the field. Moreover, the
technical concepts required to understand the rest of this thesis is introduced, and finally
the state-of-the-art in the field of authorship attribution is presented. Focus will
be on deriving authorship from binary programs; however, some related works
will be mentioned.

\section{Overview of the field}
Computer forensics is a branch where the goal is to analyse digital data to
obtain information, often in relation to computer crime
\parencite{reith2002examination}. One application is authorship attribution. In
recent years machine learning techniques have been used with great success to
identify counterfeited integrated circuits \parencite{huang2013counterfeit},
distinguishing different kinds of brain tumours
\parencite{zacharaki2009classification} and the automated detection of software
bugs \parencite{aleem2015comparative}, to name a few examples.

The first attempt at using statistical methods to de-anonymise authors of
programs from their coding style was performed by Oman et al.
\parencite{oman1989programming} who classified programmers using a hand-picked,
language-specific feature set. In 2006, Frantzeskou et al. proposed a method
using the analysis of byte level n-grams (continuous sequences of n bytes)
present in the source code which greatly improved upon previous methods, along
with being language-agnostic \parencite{frantzeskou2006source}. In 2015,
Caliskan-Islam et al. presented a method based on features of the abstract
syntax trees (ASTs) of C/C++ programs that significantly improved the
state-of-the-art again \parencite{caliskan2015anonymizing}.
 
The above methods all assume that de-anonymisation is to be performed on the
source code of a program. However, there are many contexts where
de-anonymisation is desirable where access to the source code is either
impractical or entirely impossible, for example during malware analysis. In
2011, Rosenblum et al. showed that this is feasible in certain conditions
using machine learning techniques on the stylistic features of source code that
survive the compilation process \parencite{rosenblum2011wrote}. Specifically,
their features were derived from the control flow graph (CFG) of the compiled
binary and byte level n-grams. In 2014 Alrabaee et al. improved upon the
accuracy of those techniques and proposed features that were more closely
related to programming style \parencite{alrabaee2014oba2}. In 2015,
Caliskan-Islam et al. further improved the state of the art using disassemblers
and decompilers, and applying their research in de-anonymising programmers from
source code to the ASTs of the decompiled binaries
\parencite{caliskan2015coding}.

\section{Technical Background}
This section includes the technical details necessary for understanding the
rest of this thesis. Initially it starts with a brief overview of the compilation process
and the static analysis. Moreover, techniques used to extract features from binary
executables and a short explanation of support vector machines (SVMs) and their
uses. 

\subsection{The compilation process}
Compilation is the process of transforming source code into executable
programs. It is beyond the scope of this thesis to give a complete overview of
the inner workings of compilers, so the thesis will utilise the simplification that
compilation is a two step process: assembly code generation and machine code
generation.

[Figure]

As illustrated in figure 2.TK, stylistic elements present in source code are
not easily discernible from the resulting binary code. This can be formalised
with the observation that compilation is in general non-reversible; there are
an infinite number of source code programs that would generate a given binary
executable. As such the compilation process suffers from information loss,
since information contained within the source code may not be present in the
resulting binary. Figure 2.TK shows two different source code representations
of the same binary program.

[Figure]

However, it is hard to say exactly what information is preserved in the
compilation process. The compiler guarantees that the semantics of the source
code be preserved in the resulting machine code but its algorithms and data
structures may not. Figure 2.TK shows how gcc optimises the calculation of an
arithmetic sum, replacing the loop with the constant-time formula. The results
of \parencite{rosenblum2011wrote}, \parencite{caliskan2015coding}, and
\parencite{alrabaee2014oba2} are novel in this regard as they show that
stylistic features present in source code do in fact consistently survive the
compilation process.

[Figure]
\subsection{Static analysis of compiled binaries} \label{static-analysis-tb}
Static binary analysis is the process of understanding how a binary program
will behave without executing it. This can involve any number of techniques
but is often used in the context of reverse engineering, that is, the goal of
the analysis is to understand not only the behaviour but also the
implementation of the program. It is assumed that the source code of the
program is not accessible. In this section two methods of static
analysis, disassembling and control flow analysis, that are used in the
analysis of the Google Code Jam submissions will be presented.

Disassembling is the task of transforming binary code into the equivalent
assembly code and is performed by a disassembler. In general this is a
non-trivial task, especially on machine code for complex instruction sets such
as x86 \parencite{wartell2011differentiating}. Moreover, a number of techniques
can be employed to purposely thwart commonly used disassembly algorithms,
making the process much harder \parencite{linn2003obfuscation}. For the purpose
of our analysis however this is not an issue, because
compiled Google Code Jam solutions in the data set used can in fact be automatically
disassembled. Figure 2.TK shows a simple assembly routine, its equivalent
machine code after compilation and the resulting assembly code after
disassembly.

[figure]

Control flow analysis is performed on assembly code produced by a disassembler.
The goal is to partition the code into so-called “basic blocks”: sequences of
instructions with exactly one entry point and exit point with respect to
program flow \parencite{allen1970control}. This structure is captured in the
control flow graph (CFG), a directed graph with the basic blocks as nodes and
control flow paths as edges. Figure 2.TK illustrates the basic blocks of a
simple assembly code routine.

[figure]

The CFG provides a structured view of the binary at a higher level than
individual instructions. This can be used to identify high-level language
constructs such as loops and functions \parencite{cifuentes1993methodology} and
as such is used by decompilers such as the Hex-Rays decompiler (TK: Hex-Rays,
2016), used by Caliskan-Islam et al. in their paper improving on the work of
Rosenblum et al \parencite{caliskan2015coding}. Rosenblum et al. also use the
CFG for feature extraction, but do not go as far as to decompile the binaries
\parencite{rosenblum2011wrote}.

\subsection{Further abstractions of the CFG}
In their 2011 paper on authorship attribution, Roesenblum et al. defined an
additional two abstractions based on the CFG of a program: the \emph{coloured
control flow graph} (CCFG) and the \emph{coloured call graph} (CCG).

The CCFG is a graph identical to the CFG with an additional \emph{colouring
function}. Each instruction in the target instruction set is classified as one
of fourteen instruction classes such as ``arithmetic instructions'' or
``branching instructions'', and a node's colour is simply a 14 bit bitmap
representing which instruction types are present in its corresponding basic
block. The colouring process is described in more detail in
\parencite{rosenblum2011recovering}, but further knowledge is not needed for
the rest of this thesis.

The CCG is created from the CFG by only including the subset of nodes that contain
a call instruction, with edges between two nodes representing a path between
the corresponding nodes in the original CFG. A colouring function is applied in
the same way as for the CCFG, with the colours depending on whether not the
node calls a library function, and if so, which library function is called.

\subsection{Support vector machines}
Support vector machines (SVMs) belong to a group of supervised learning methods
and are based on statistical learning theory. SVMs have been around since the
1960s and work well on for example text and image recognition. The learning
algorithm is given data and learns from that to identify patterns or classify
new data. There is a balance in how much data the algorithm should have. Using
too much information can lead to “overfitting” and the algorithm will perform
poorer for new data (TK: Weston, n.d.). The goal is to maximize the distance
between data points in the different classes \parencite{awad2004effective}.

\subsection{Cross-validation}
Cross-validation is a common method used when testing predictions. It is a
model used to analyze how well a predictive model will work in practice. When
using 10-fold cross-validation the data is randomly divided into 10 equally
sized sets, 9 of the sets is used for training and the remaining set is used
for the testing. This is repeated for each one of the sets, finally resulting
in an average measure of accuracy \parencite{hsu2003practical}.        

\section{State-of-the-art analysis}
As previously discussed, there are as far as we are aware of exactly three
papers presenting novel techniques to de-anonymise programmers from compiled
binary programs: \parencite{rosenblum2011wrote}, \parencite{alrabaee2014oba2}
and \parencite{caliskan2015coding}. In this section each approach will be discussed.
The focus will be on the techniques used by Rosenblum et al.

\subsection{Rosenblum et al.}
Rosenblum et al. evaluate their data on both a private data set obtained from
an operating systems course (CS537) at the University of Wisconsin and a subset
of correct solutions from the 2009 and 2010 rounds of the Google Code Jam
(GCJ), specifically the solutions that:

\begin{itemize}
\item Were written in C/C++
\item Could be compiled using GCC 4.5 
\item Were submitted by an author that had submitted correct solutions to at
      least 8 problems in total
\end{itemize}

%TK: This thesis only care about 2010 data
In total, the GCJ data set consisted of 2581 binaries (1,747 binaries from
2010) from 284 authors. Using 10-fold cross-validation on a set of 191 authors
from 2010, 51\% accuracy was achieved. For a randomly selected subset of 10
authors, 81\% accuracy was achieved and for a randomly selected subset of 20
authors 77\% accuracy.

\subsection{Alrabaee et al.}
Another study by Alrabaee et al. (An Onion approach to Binary code Authorship
Attribution) used an approach they called OBA2 to identify author style in
binary code \parencite{alrabaee2014oba2}. The study refers to the work of
Rosenblum et al. and argues that some of the features used in Rosenblum et al.
do not represent author style features. The result of the study (diagrams)
shows a higher accuracy than \parencite{rosenblum2011wrote}.The data used in
the study was the same as Rosenblum et al. 

\subsection{Caliskan-Islam et al.}
Another report published in December 2015 by Caliskan-Islam et al. improves on
the work of Rosenblum. This study has been able to improve author
identification accuracy even further also using data from Google Code Jam and
c/c++ binary code. With a set of 20 authors they are able to find the correct
author with 96\% accuracy. Caliskan-Islam (2015) also tested the effect of
different optimisations and got with 100 authors and no optimisation 78.3\%
accuracy and with level 3 optimisation 60.1\% accuracy. Furthermore, the study
found that it was easier to identify more experienced programmers.

\chapter{Method}
\section{Data and datasets}
The analysis is based on data collected from the 2010 round of Google Code Jam
(GCJ). Specifically, the analysis is restricted to authors that have submitted at
least 8 submissions in C++. Furthermore, it is also required that solutions consist of exactly
one file with the extension .cpp which can be compiled and statically linked
with g++ in the test environment used (see section 3.2 for details).

Three subsets with 20, 191, and all 413 authors of the GCJ 2010 data set are
used in order to test how the size of training data affects our analysis.
Furthermore, analysis on binaries compiled with the -static flag is
computationally infeasible with large datasets in our test environments. This
is summarised in Table~\ref{tab:datasets-summary}.

\begin{table}[!htb]
    \centering
        \begin{tabular}{ l l l }
        Authors & Subsets & Compiler flags \\ \hline
        413 & 413, 20 &  \lstinline{-O0, -O1, -O2, -O3} \\ 
        191 & 191, 20, 10 &  \lstinline{-O0, -O3} \\ 
        20 & 20, 10 &  \lstinline{-O0 static} 
        \end{tabular}
    \caption{Summary of subsets of the 2010 GCJ dataset used in our analysis}
    \label{tab:datasets-summary}
\end{table}

These dataset is chosen primarily to allow us to easily compare our results
with the existing literature; however, it should be said that the structure of
the GCJ dataset is a good fit for our analysis nonetheless. An
unsupervised classification algorithm is used with a large number of features; thus, it is
important to choose features derived from the coding style of the programmer
and not the problem itself. The GCJ dataset facilitates this since all
contestants solve the same set of problems; features derived from the problem
formulation perform poorly as classifiers and can be identified and discarded.
Likewise, features derived from the compilation process itself can also be
identified and discarded due to all solutions being compiled in the same way.

\section{Test environment}
All processing was done on an server running Fedora 23 x64.
Table~\ref{tab:dnf-packages} lists the dnf packages used in our test
environment that are not included in the base installation used:

\begin{table}[!htb]
    \centering
        \begin{tabular}{ l l l }
        Package name & Version & Usage \\ \hline
        gcc-c++ & 5.3.1-2.fc23 & Compiling GCJ submissions \\
        python & 2.7.10-8.fc23 & Scripting, data manipulation \\
        libsvm & 3.20-5.fc23  & Support vector machine library \\
        libsvm-python & 3.20-5.fc23 & Python bindings for libsvm
        % TK: This is incomplete
        \end{tabular}
    \caption{\lstinline{dnf} packages not including in the base Fedora 23 x64
    installation that are used in our analysis}
    \label{tab:dnf-packages}
\end{table}

In a supplement to their 2011 paper, Rosenblum et al. provided the programs
they created for binary feature extraction as open source software. The same programs is used for our own analysis, albeit with minor changes due to
outdated dependencies\footnote{
    The original feature extraction code can be found at
    \url{http://pages.cs.wisc.edu/~nater/esorics-supp/}. Our modified versions
    can be found at \url{https://github.com/macaullyjames/esorics}, along with
    installation instructions.
}.

\section{Feature extraction}
Feature extraction is the process of extracting measurable properties from the
sample dataset for further analysis. This section aims to provide a short
account of the \emph{feature templates} used by Rosenblum et al., on which this
thesis is based upon. For an in-depth account of how each feature template contributes
to the overall analysis, we refer to their original paper
\parencite{rosenblum2011wrote}.

Becasue it is not known which properties of the compiled binaries are
meaningful for determining authorship, the approach of Rosenblum et al. is to
extract a large number of generic features in the hope that authorship
information will be captured indirectly. All features are derived from the
program's disassembled code, coloured control flow graph (CCFG) and coloured
call graph (CCG) (section \ref{static-analysis-tb} explains these concepts in
detail) and are based on 6 feature templates: 

\begin{description}[style=nextline]
\item[Idioms]
All sequences of 1-3 instructions are extracted and counted from the
disassembled code. This is performed intraprocedurally; only sequences of
instructions within the same procedure are considered. Wildcard instructions
are allowed in order to capture more general instruction patterns.

\item[N-grams]
N-grams (continuous sequences of bytes) of length 3 or 4 in are extracted from
the program. Again, this is performed intraprocedurally so n-grams outside of
procedures will be ignored.

\item[Graphlets]
All connected components of size 3 of the CCFG are extracted.

\item[Supergraphlets]
Each node in the CCFG is collapsed with a random neighbour and all connected
components of size 3 of the resulting graph are extracted. The algorithm used
to collapse nodes is described in detail in \parencite{rosenblum2011recovering} 

\item[Call graphlets]
All connected components of size 3 of the CCG are extracted.

\item[External interaction]
The name of each library routine used by the program is extracted.
\end{description}

Features are extracted for each program in the dataset and the set of unique
features in the entire dataset is generated; each feature can then be indexed
and recognised across programs. Finally, features are grouped by author and
saved in the LibSVM format %TK: reference
for analysis. Tables \ref{tab:413-features},
\ref{tab:191-features} and \ref{tab:20-features} show the number of unique
features as well as the total number of features found in our datasets of sizes
413, 191 and 20 respectively.   

\begin{table}[!htb]
    \centering
        \begin{tabular}{ l l l }
        Compiler setting & Unique features & Total features \\ \hline
        \lstinline{-O0} & 765307 & 13204201 \\
        \lstinline{-O3} & 3891494 & 21570182
        \end{tabular}
    \caption{Features in the 413 author dataset}
    \label{tab:413-features}
\end{table}

\begin{table}[!htb]
    \centering
        \begin{tabular}{ l l l }
        Compiler setting & Unique features & Total features \\ \hline
        \lstinline{-O0} & 509908 & 6221646 \\
        \lstinline{-O3} & 2380127 & 10276979
        \end{tabular}
    \caption{Features in the 191 author dataset}
    \label{tab:191-features}
\end{table}

\begin{table}[!htb]
    \centering
        \begin{tabular}{ l l l }
        Compiler setting & Unique features & Total features \\ \hline
        \lstinline{-O0 -static} & 3859605 & 110725073 \\
        \end{tabular}
    \caption{Features in the 20 author dataset}
    \label{tab:20-features}
\end{table}

\section{Feature selection}
The feature extraction process yields a high dimensional dataset with hundreds
of thousands, sometimes millions, of features. In order to speed up the
training of the SVM and avoid overfitting it is desirable to remove features
that provide little or no information about the authors’ programming style. To
achieve this, features are ranked according to the mutual information
criteria \parencite{guyon2003introduction} and then a model trained
with varying sizes of subsets of the highest ranking features is evaluated. The model with
the best predictive power is retained.

\subsection{Ranking using mutual information}
Let $X$ be the multiset of all features generated for a given data set. For all
$x \in X$, let $P(X = x)$ denote the observed frequency of $x$ in $X$, that is,
$P(X = x) = \frac{1}{|X|}$. Likewise, let $Y$ be the set of all author labels
and $P(Y = y)= \frac{1}{|Y|}$ be the observed frequency of the label $y$ in $Y$
and let $P(X=x_i,Y=y)$ denote the joint frequency of $x_i$ and $y$ in the data
set. Each unique feature is ranked $x_i \in supp X $ using the mutual information
criteria $R(x_i)$ (Guyon, 2003) in the following way:

$$R(x_i)= \sum_{x_i \in X, y \in Y} P(X=x_i,Y=y) \log \frac{P(X = x_i,
Y = y)}{P(X = x_i)P(Y = y)}$$

Intuitively the highly ranked features will be unique to a small set of author
labels and as such more useful in the classification process, whereas the
lowest ranked features can be removed without negatively affecting the
predictive power of the model.
 
Determining the optimal subset of features
 
Using the above ranking algorithm, a model is trained using the 800, 1900, 7000,
8000, 9000, 10000 and 100000 highest ranked features, choosing the subset of
features that yields the best predictive power. When computationally feasible, models
trained with all features have also been compared.
The predictive power of each model is validated through cross-validation, see
section TK for details.

\section{Author classification}
As in the original paper from Rosenblum et al., classification is performed
using a linear support vector machine (SVM) \parencite{rosenblum2011wrote}. See
section 2.TK for details. The svm-scale program from the LIBSVM toolkit is used
\parencite{chang2011libsvm} to scale the feature values (avoids overweighting
frequently occurring features) to the interval [0, 1]:
\begin{lstlisting}
svm-scale -l 0 -u 1 features.dat > features.scale
\end{lstlisting}

The LIBLINEAR SVM implementation is then used \parencite{fan2008liblinear} for
training and evaluating via 10-fold cross validation:
\begin{lstlisting}
train -v 10 features.scale
\end{lstlisting}


\chapter{Results}
This section present the results from our different experiments using 10-fold
cross-validation to obtain the accuracy values. The experiments vary in number
of authors used, total number of features and processing. The purpose is to
show how the accuracy varies in different scenarios.  

\section{Test with 413 authors and different optimisation levels}
This experiment uses a dataset of 413 authors and optimisation levels -O0 to
-O3. Moreover, this experiment examine the accuracy when using different
numbers of top ranked features in the training dataset in an attempt to find a
maximum value. To get more accurate values the average of many test instances
is used. In this case 20 subsets with data from 20 randomly selected authors
from the total of 413 authors were used.

\subsection{With all 413 authors and optimisation level -O0 to -O3}
Figure 4.1. This diagram sums up the results of using the entire set of 413
authors with different optimisation levels and top ranked features.
Optimisation with -O0 gave the highest accuracy 32.53\% using 7000 features.
Optimisation with -O1 gave its highest accuracy 28.60\% using 9000 features.
Optimisation with -O2 gave its highest accuracy 28.60\% using 10000 features.
Optimisation with -O3 gave its highest accuracy 21.63\% using 10000 features. 


\subsection{Subset with 20 authors and optimisation level -O0}
Figure 4.2. This diagram shows the results of binaries that were compiled with
optimisation level -O0 and had a total of 765307 unique features. The x-axis
shows the number of top ranked features used  and the y-axis shows the average
of the results from the 20 subsets used. The highest accuracy average achieved
was 68.80\% with 8000 features. 

Figure 4.3. In this diagram the result of every instance of the 20 instances
using 8000 features and with 20 randomly selected authors is displayed.
Standard deviation is 5.18\%. 

\subsection{Subset with 20 authors and optimisation level -O1}
Figure 4.4. This diagram shows the results of binaries that were compiled with
optimisation level -O1 and had a total of 2141558 unique features. The x-axis
shows the number of top ranked features used and the y-axis shows the average
of the results from the 20 subsets used. Highest accuracy average achieved was
65.42\% with 7000 features. 


Figure 4.5. In this diagram the result of every instance of the 20 instances
using 7000 features and with 20 randomly selected authors is displayed.
Standard deviation is 6.86\%.

\subsection{Subset with 20 authors and optimisation level -O2}
Figure 4.6. This diagram shows the results of binaries that were compiled with
optimisation level -O2 and had a total of 2593340 unique features. The x-axis
shows the number of top ranked features used and the y-axis shows the average
of the results from the 20 subsets used. Highest accuracy average achieved was
64.43\% with 9000 features.

Figure 4.7. In this diagram the result of every instance of the 20 instances
using 9000 features and with 20 randomly selected authors is displayed.
Standard deviation is 6.99\%.

\subsection{Subset with 20 authors and optimisation level -O3}
Figure 4.8. This diagram shows the results of binaries that were compiled with
optimisation level -O3 and had a total of 3891394 unique features. The x-axis
shows the number of top ranked features used and the y-axis shows the average
of the results from the 20 subsets used. Highest accuracy average achieved was
58.66\% with 9000 features.

Figure 4.9. In this diagram the result of every instance of the 20 instances
using 9000 features and with 20 randomly selected authors is displayed.
Standard deviation is 5.73\%.

\subsection{Summary}
Figure 4.X. Comparison of results from the different optimisations levels using
staples.  

Figure 4.X. Comparison of results from the different optimisations levels using
lines.  

\section{Test with 191 authors and different optimisation levels}
This experiment uses a dataset of 191 authors and optimisation levels -O0 and
-O3. It shows how fewer authors affect the results and is the same number of
authors used in Rosenblum et al. However, the exact set of authors may be
different and the number of files is not the same. Moreover, this experiment
examine the accuracy when using different numbers of top ranked features in the
training dataset in an attempt to find a maximum value. To get more accurate
values the average of many test instances is used. In this case 20 subsets with
data from 20 randomly selected authors from the total of 191 authors were used.
In addition, tests with 10 authors instead of 20 was also used.  

\subsection{With all 191 authors and optimisation level -O0 and -O3}
Figure 4.X: This diagram sums up the results of using the entire set of 191
authors with optimisation levels -O0, -O3 and different numbers of top ranked
features. Optimisation with -O0 gave the highest accuracy 41.80\% using 7000
features. Optimisation with -O3 gave its highest accuracy 29.61\% using 10000
features. 

\subsection{Subset with 20 authors and optimisation level -O0}
Figure 4.X. This diagram shows the results of binaries that were compiled with
optimisation level -O0 and had a total of 509908 unique features. The x-axis
shows the number of top ranked features used  and the y-axis shows the average
of the results from the 20 subsets used. The highest accuracy average achieved
was 71.22\% with 8000 features.

Figure 4.X.  In this diagram the result of every instance of the 20 instances
using 8000 features and with 20 randomly selected authors is displayed.
Standard deviation is 6.18\%.

\subsection{Subset with 20 authors and optimisation level -O3}
Figure 4.X. This diagram shows the results of binaries that were compiled with
optimisation level -O3 and had a total of 2380127 unique features. The x-axis
shows the number of top ranked features used and the y-axis shows the average
of the results from the 20 subsets used. Highest accuracy average achieved was
59.79\% with 9000 features.

Figure 4.X.  In this diagram the result of every instance of the 20 instances
using 9000 features and with 20 randomly selected authors is displayed.
Standard deviation is 5.66\%.

\subsection{Subset with 10 authors and optimisation level -O0}
Figure 4.X.  This diagram shows the results of binaries that were compiled
with optimisation level -O0 and had a total of 509908 unique features. The
x-axis shows the number of top ranked features used and the y-axis shows the
average of the results from the 20 subsets used, this time with 10 authors.
Highest accuracy average achieved was 81.58\% with 8000 features.

Figure 4.X. In this diagram the result of every instance of the 0 instances
using 8000 features and with 10 randomly selected authors is displayed.
Standard deviation is 7.09\%.

\subsection{Subset with 10 authors and optimisation level -O3}
Figure 4.X. This diagram shows the results of binaries that were compiled with
optimisation level -O3 and had a total of 2380127 unique features. The x-axis
shows the number of top ranked features used and the y-axis shows the average
of the results from the 20 subsets used, this time with 10 authors. Highest
accuracy average achieved was 73.22\% with 10000 features.

Figure 4.X. In this diagram the result of every instance of the 20 instances
using 10000 features and with 10 randomly selected authors is displayed.
Standard deviation is 9.24\%.

\subsection{Summary}
Figure 4.X. Comparison of results from the different optimisations levels and
20 authors.

Figure 4.X. Comparison of results from the different optimisations levels and
10 authors.

Figure 4.X. Comparison of results with different number of authors using
staples.

Figure 4.X. Comparison of results with different number of authors using
lines.

\section{Test with 20 authors and static compilation}
This experiment uses a dataset of 20 authors and without  optimisation. It
shows how the use of static affect the results. Moreover, this experiment
examine the accuracy when using different numbers of top ranked features in the
training dataset in an attempt to find a maximum value.  To get more accurate
values the average of many test instances is used. In this case 20 subsets with
data from 10 randomly selected authors from the total of 20 authors were used.
In addition, tests with each data file with 20 authors was used; however, no
average could be used. Tests with all features were not possible or took too
long time to obtain in this case.   

\subsection{With all 20 authors and static (optimisation level -O0)}
Figure 4.X. This diagram shows the results of binaries that were compiled with
static and had a total of 3859605 unique features. The x-axis shows the number
of top ranked features used and the y-axis shows the the results from a single
set with 20 authors. Highest accuracy achieved was 28.65\% with 9000 features.

\subsection{Subset with 10 authors and static (optimisation level -O0)}
Figure 4.X. This diagram shows the results of binaries that were compiled with
static and had a total of 3859605 unique features. The x-axis shows the number
of top ranked features used and the y-axis shows the average of the results
from the 20 subsets used, this time with 10 authors. Highest accuracy average
achieved was 45.54\% with 8000 features.

Figure 4.X. In this diagram the result of every instance of the 20 instances
using 8000 features and with 10 randomly selected authors is displayed.
Standard deviation is 6.62\%.

\subsection{Summary}
Figure 4.X. Comparison of results with different number of authors using
staples.

Figure 4.X. Comparison of results with different number of authors using lines.


\chapter{Discussion}
\section{Method discussion}
When attempting to implement the method used in Rosenblum et al. it was not
without difficulties. To be able to use the source code provided by Rosenblum
et al. some adjustments to the source code had to be made, updating it to make
it work on a more modern system. As far as we can tell it has not caused any
errors.

The source code provided by Rosenblum et al. was only for feature extraction.
Therefore, it is possible that inconsistencies with the method of Rosenblum et al. could have
emerged from the process of converting output data from the feature extraction to
data with the LIBSVM format needed for LIBLINEAR.

Something to notice is that more features than Rosenblum et al. was needed to achieve high
accuracy; thus, our implementation of feature selection may differ from that of
Rosenblum et al. 

\section{Results analysis}
The experiments conducted show that optimisation reduces the accuracy. This is
similar to what \parencite{caliskan2015coding} found in their experiments. As
previously mentioned, they got with 100 authors and no optimisation 78.3\%
accuracy and with level 3 optimisation 60.1\% accuracy.

When optimisation is used to compile the binaries there tend to be a larger
amount of features extracted. Optimisation with -O3 gave 3891494 unique
features in the case with 413 authors compared to 736307 features when -O0 was
used. That is more than five times as many features. For optimisation -O3  the
highest accuracy with 20 authors was 58.66\% compared to 68.80\% when no
optimisation was used (see Figure 4.2. and Figure 4.8.), a 10.14\% difference).
Furthermore, optimisation with -O1 and -O2 gave only a small reduction in
accuracy. The results are still much higher than the random chance 5\% of
guessing the author . 

Experiments with subsets of 20 authors from the set of 191 authors shows
similar results. When no optimisation was used the highest accuracy was 71.22
\% compared to 59.79\% when -O3 was used (see Figure 4.X. and  Figure 4.X. ).
In this case the difference is 11.43\%.

It is also possible to see that the accuracy is higher when fewer authors is
used which is not that strange, random chance of guessing the correct author is
also higher 10\%. If the case with 20 authors and no optimisation from the set
of 191 is compared with that of 10 authors (see Figure 4.X.) the highest
accuracy with 10 authors and was 81.58\% compared to 71.22\% with 20 authors.
The difference is 10.36\%.

When using static with only 20 authors the unique number of features was
3859605 and the total number of features was 110725073. In contrast, even
though using more authors,  the number of unique features from the set of 191
authors was 509908 and the total number of features was only 6221646.The
accuracy achieved in the case using static linking and 10 authors was 45.54\%
compare to 81.58\%. Thus, an important factor affecting the accuracy seems to
be the number of features. 

Even though it was not possible to replicate Rosenblum’s et al. (2011) experiment and data
exactly, our experiments showed accuracy levels not far from theirs. Using 191
authors and 20 authors \parencite{rosenblum2011wrote} got approximately 77\% while
our experiment using 191 authors gave 71\%; however, with a different number of files and
probably not the exact set of 191 authors.     

With no optimisation and using all 191 authors and 7000 features an accuracy of
41.8\% was achieved (see Figure 4.23). It is much better than random chance
0.5\%. \parencite{rosenblum2011wrote} achieved 51\% on the full set with 191
authors using only 1900 features. Not only did they achieve better results,
they also manage to do this using fewer features. Which could indicate that our
implementation of feature selection differ from that of Rosenblum et al.

\section{Future research}
There are more techniques one could use when processing the source code or the
binaries that could be interesting to investigate further. It would be
interesting to test how different packers affect the accuracy. Moreover, it
would be interesting to see how this method would work in more practical
situations. Is it possible to identify teams using these techniques? What if
the team’s members change frequently? What if two teams enforce the same style
guide? In real situations the code could be processed using different methods.
What would happen if the binaries were compiled with a mix of different
optimisation levels and static?  

Most of the previous reports have focus on code from the Google Code Jam
programming competition. More research on other datasets with larger files
could be interesting to examine.

With the purpose of testing the accuracy values obtained it could also be
interesting to use different machine learning approaches. Possible try another
program than LIBLINEAR for the cross-validation.

\chapter{Conclusion}
This thesis attempted to verify the authorship attribution method created by
\parencite{rosenblum2011wrote}. Our results tend to be somewhat lower than that
of Rosenblum et al. and need more features to give high accuracy; nevertheless,
the method seems to be working. Some possible explanations concerning the
differences in accuracy achieved could be because of difficulties in
implementing Rosenblum et al. method exactly. Furthermore, even though working
with data from Google Code Jam 2010 the exact dataset that Rosenblum et al.
used could not be replicated and therefore cause differences in results. 

Optimisation seems not be enough to make authors anonymous; however, it reduces
the accuracy somewhat. Adding static causes a bigger drop in accuracy. The
accuracy achieved in the case using static and 10 authors was 45.54\% compare
to 81.58\% without. Perhaps not enough to ensure anonymity; nevertheless,
static gives a significantly lower accuracy. The data extracted shows that
optimisation and static linking results in more features that may be the cause
of the reduction in accuracy.

\printbibheading
\printbibliography

\end{document}
